{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmdbsimple as tmdb\n",
    "tmdb.API_KEY = '5e3a0224faa49e33a468c4948a547e2c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imdb import IMDb\n",
    "ia = IMDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rotten_tomatoes_client as rtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['release_date']=pd.to_datetime(train_set['release_date'], format='%m/%d/%y')\n",
    "test_set['release_date']=pd.to_datetime(test_set['release_date'], format='%m/%d/%y')\n",
    "train_set['release_year']=train_set['release_date'].dt.year\n",
    "train_set['release_month']=train_set['release_date'].dt.month\n",
    "test_set['release_year']=test_set['release_date'].dt.year\n",
    "test_set['release_month']=test_set['release_date'].dt.month\n",
    "test_set.at[828, 'release_month'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixyear(setname, colname='release_year'):\n",
    "    for i in range(len(setname[colname])):\n",
    "        try:\n",
    "            s_result = ia.get_movie(setname.imdb_id[i][2:])\n",
    "            the_unt = s_result\n",
    "            ia.update(the_unt)\n",
    "            year = the_unt['year']\n",
    "            if setname.release_year[i]!=year:\n",
    "                setname.at[i,'release_year']=year\n",
    "        except:\n",
    "            try:\n",
    "                result = rtc.RottenTomatoesClient.search(term='Whiplash')\n",
    "\n",
    "                for l in range(len(result)):\n",
    "                    if eval(setname.loc[setname.title==setname.title[i]].cast.tolist()[0])[0]['name'] in result['movies'][l]['subline'].split(','):\n",
    "                        ix=l\n",
    "                year=result['movies'][ix]['year']\n",
    "                if setname.release_year[i]!=year:\n",
    "                    setname.at[i,'release_year']=year\n",
    "            except Exception as e:\n",
    "                print(setname.title[i])\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixyear(train_set)\n",
    "fixyear(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.release_year.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedrating (setname, colname='title'):\n",
    "    rating=[]\n",
    "    titlelist = setname.title.tolist()\n",
    "    for i in range(len(setname[colname])):\n",
    "        counter = titlelist.count(setname.title[i])\n",
    "        if counter==1:\n",
    "            try:\n",
    "                result = rtc.RottenTomatoesClient.search(term=setname[colname][i])\n",
    "                rating.append(result['movies'][0]['meterScore']/10)\n",
    "            except:\n",
    "                try:\n",
    "                    s_result = ia.get_movie(setname.imdb_id[i][2:])\n",
    "                    the_unt = s_result\n",
    "                    ia.update(the_unt)\n",
    "                    rating.append(the_unt['rating'])\n",
    "                except:\n",
    "                    try:\n",
    "                        movie_id = setname.loc[setname.title==setname[colname][i]]['id'].tolist()[0]\n",
    "                        movie = tmdb.Movies(movie_id)\n",
    "                        response = movie.info()\n",
    "                        rating.append(movie.vote_average)\n",
    "                    except:\n",
    "                        try:\n",
    "                            s_result = ia.search_movie(setname[colname][i])\n",
    "                            the_unt = s_result[l]\n",
    "                            ia.update(the_unt)\n",
    "                            rating.append(the_unt['rating'])\n",
    "                        except:\n",
    "                            try:\n",
    "                                search = tmdb.Search()\n",
    "                                response = search.movie(query=setname[colname][i])\n",
    "                                for l in range(10):\n",
    "                                    try:\n",
    "                                        if search.results[l]['id']==setname.at[i,'id']:\n",
    "                                            rating.append(search.results[0]['vote_average'])\n",
    "                                    except:\n",
    "                                        break\n",
    "                            except Exception as e:\n",
    "                                print(titles)\n",
    "                                print(e)\n",
    "        else:\n",
    "            try:\n",
    "                s_result = ia.get_movie(setname.imdb_id[i][2:])\n",
    "                the_unt = s_result\n",
    "                ia.update(the_unt)\n",
    "                rating.append(the_unt['rating'])\n",
    "            except:\n",
    "                try:\n",
    "                    movie_id = setname.loc[setname.title==setname[colname][i]]['id'].tolist()[0]\n",
    "                    movie = tmdb.Movies(movie_id)\n",
    "                    response = movie.info()\n",
    "                    rating.append(movie.vote_average)\n",
    "                except:\n",
    "                    try:\n",
    "                        s_result = ia.search_movie(setname[colname][i])\n",
    "                        the_unt = s_result[l]\n",
    "                        ia.update(the_unt)\n",
    "                        rating.append(the_unt['rating'])\n",
    "                    except:\n",
    "                        try:\n",
    "                            search = tmdb.Search()\n",
    "                            response = search.movie(query=setname[colname][i])\n",
    "                            for l in range(10):\n",
    "                                try:\n",
    "                                    if search.results[l]['id']==setname.at[i,'id']:\n",
    "                                        rating.append(search.results[0]['vote_average'])\n",
    "                                except:\n",
    "                                    continue\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(titles)\n",
    "                            print(e)\n",
    "    return rating     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating = weightedrating(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rating = weightedrating(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['rating'] = train_rating\n",
    "test_set['rating'] = test_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.at[2398,'rating'] = 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/Users/m-sim/Desktop/train_set.pkl', 'wb')\n",
    "pickle.dump(train_set, f)\n",
    "f.close()\n",
    "f = open('C:/Users/m-sim/Desktop/test_set.pkl', 'wb')\n",
    "pickle.dump(test_set, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/Users/m-sim/Desktop/train_set.pkl','rb')\n",
    "train_set = pickle.load(f)\n",
    "f.close()\n",
    "f = open('C:/Users/m-sim/Desktop/test_set.pkl','rb')\n",
    "test_set = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.at[828, 'release_month'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/Users/m-sim/Desktop/variables/train_year_rating.pkl', 'wb')\n",
    "pickle.dump([train_set.release_year.tolist(),train_set.rating.tolist()], f)\n",
    "f.close()\n",
    "f = open('C:/Users/m-sim/Desktop/variables/test_year_rating.pkl', 'wb')\n",
    "pickle.dump([test_set.release_year.tolist(),test_set.rating.tolist()], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urlopen('https://www.rottentomatoes.com/m/whiplash_2014')\n",
    "\n",
    "soup = bs(response, 'lxml')\n",
    "\n",
    "aud_rating = soup.find(\"span\", {\"class\": \"mop-ratings-wrap__percentage mop-ratings-wrap__percentage--audience\"}).get_text(strip=True)\n",
    "aud_rating = int(aud_rating.split('%')[0])/10\n",
    "aud_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rtc.RottenTomatoesClient.search(term='rocky')\n",
    "result['movies'][0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def aud_rate(setname,colname='title'):\n",
    "    aud_rating_list=[]\n",
    "    for i in range(len(setname[colname])):\n",
    "        try:\n",
    "            title = '_'.join(str(setname[colname][i]).lower().translate(str.maketrans('', '', string.punctuation)).split(' '))\n",
    "            response = urlopen('https://www.rottentomatoes.com/m/'+title)\n",
    "\n",
    "            soup = bs(response, 'lxml')\n",
    "\n",
    "            aud_rating = soup.find(\"span\", {\"class\": \"mop-ratings-wrap__percentage mop-ratings-wrap__percentage--audience\"}).get_text(strip=True)\n",
    "            aud_rating = int(aud_rating.split('%')[0])/10\n",
    "            aud_rating_list.append(aud_rating)\n",
    "        except:\n",
    "            try:\n",
    "                title = '_'.join(str(setname[colname][i]).lower().translate(str.maketrans('', '', string.punctuation)).split(' '))\n",
    "                year = str(setname.loc[setname.title==setname[colname][i]]['release_year'].tolist()[0])\n",
    "                ty = '_'.join([title,year])\n",
    "                response = urlopen('https://www.rottentomatoes.com/m/'+ty)\n",
    "\n",
    "                soup = bs(response, 'lxml')\n",
    "\n",
    "                aud_rating = soup.find(\"span\", {\"class\": \"mop-ratings-wrap__percentage mop-ratings-wrap__percentage--audience\"}).get_text(strip=True)\n",
    "                aud_rating = int(aud_rating.split('%')[0])/10\n",
    "                aud_rating_list.append(aud_rating)\n",
    "            except:\n",
    "                try:\n",
    "                    result = rtc.RottenTomatoesClient.search(term=setname[colname][i])\n",
    "                    for l in range(int(len(result)/10)):\n",
    "                        if eval(setname.loc[setname.title==setname.title[i]].cast.tolist()[0])[0]['name'] in result['movies'][l]['subline'].split(','):\n",
    "                            response = urlopen('https://www.rottentomatoes.com'+result['movies'][l]['url'])\n",
    "                            soup = bs(response, 'lxml')\n",
    "\n",
    "                            aud_rating = soup.find(\"span\", {\"class\": \"mop-ratings-wrap__percentage mop-ratings-wrap__percentage--audience\"}).get_text(strip=True)\n",
    "                            aud_rating = int(aud_rating.split('%')[0])/10\n",
    "                            aud_rating_list.append(aud_rating)\n",
    "                        elif setname.at[i,'release_year']==result['movies'][l]['year']:\n",
    "                            response = urlopen('https://www.rottentomatoes.com'+result['movies'][l]['url'])\n",
    "                            soup = bs(response, 'lxml')\n",
    "\n",
    "                            aud_rating = soup.find(\"span\", {\"class\": \"mop-ratings-wrap__percentage mop-ratings-wrap__percentage--audience\"}).get_text(strip=True)\n",
    "                            aud_rating = int(aud_rating.split('%')[0])/10\n",
    "                            aud_rating_list.append(aud_rating)\n",
    "                        else:\n",
    "                            aud_rating_list.append(setname.loc[setname.title==setname[colname][i]]['rating'].tolist()[0])\n",
    "                except:\n",
    "                    print(setname[colname][i])\n",
    "                    print(i)\n",
    "                    aud_rating_list.append(setname.loc[setname.title==setname[colname][i]]['rating'].tolist()[0])\n",
    "    return aud_rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Here's What's Happening to Me\n",
      "95\n",
      "Sällskapsresan - eller finns det svenskt kaffe på grisfesten?\n",
      "218\n",
      "Gary Gulman: It's About Time\n",
      "224\n",
      "The National Shotgun\n",
      "237\n",
      "Would I Lie to You? 2\n",
      "250\n",
      "Delhi Dance\n",
      "253\n",
      "Voracious\n",
      "254\n",
      "The Inhabited Island 2: Rebellion\n",
      "295\n",
      "Asterix & Obelix Take on Caesar\n",
      "367\n",
      "Dear Guest, When Will You Leave?\n",
      "419\n",
      "The Book of Mormon Movie, Volume 1: The Journey\n",
      "470\n",
      "Nikitich and The Dragon\n",
      "487\n",
      "Alaipayuthey\n",
      "516\n",
      "El pico 2\n",
      "564\n",
      "Katyar Kaljat Ghusali\n",
      "586\n",
      "А поутру они проснулись\n",
      "591\n",
      "A Prince (almost) Charming\n",
      "595\n",
      "The Day After Tomorrow\n",
      "680\n",
      "Ivan Tsarevich & the Grey Wolf 2\n",
      "884\n",
      "In the Forests of Siberia\n",
      "911\n",
      "O Kadhal Kanmani\n",
      "918\n",
      "Pokémon 4Ever: Celebi - Voice of the Forest\n",
      "943\n",
      "Fuck You Goethe 2\n",
      "960\n",
      "The Broken Hearts Club: A Romantic Comedy\n",
      "981\n",
      "Zyzzyx Road\n",
      "1006\n",
      "Billy Gardell: Halftime\n",
      "1007\n",
      "Stepfather II: Make Room For Daddy\n",
      "1046\n",
      "FBI: Frikis buscan incordiar\n",
      "1053\n",
      "Crazy About Ya\n",
      "1092\n",
      "El robobo de la jojoya\n",
      "1100\n",
      "Pokémon: Arceus and the Jewel of Life\n",
      "1136\n",
      "Candy Razors\n",
      "1186\n",
      "Atlas Shrugged Part III: Who is John Galt?\n",
      "1197\n",
      "Melissa P.\n",
      "1206\n",
      "Cry, Onion!\n",
      "1255\n",
      "Pokémon: Spell of the Unknown\n",
      "1266\n",
      "Queen of Spades: The Dark Rite\n",
      "1279\n",
      "Jaws 3-D\n",
      "1295\n",
      "Elections Day 2\n",
      "1406\n",
      "My Left Foot: The Story of Christy Brown\n",
      "1433\n",
      "Under Electric Clouds\n",
      "1453\n",
      "The Irony of Fate. The Sequel\n",
      "1464\n",
      "Alesha Popovich and Tugarin the Dragon\n",
      "1470\n",
      "Love and the City 2\n",
      "1488\n",
      "Rescue Under Fire\n",
      "1490\n",
      "Petersburg: Only for Love\n",
      "1522\n",
      "All at Once\n",
      "1541\n",
      "Varalaru\n",
      "1548\n",
      "Red Cliff Part II\n",
      "1579\n",
      "The Tulse Luper Suitcases, Part 1: The Moab Story\n",
      "1581\n",
      "The Assassination of Richard Nixon\n",
      "1617\n",
      "Tri bogatyrya i Shamakhanskaya tsaritsa\n",
      "1632\n",
      "Code Name: Jackal\n",
      "1654\n",
      "Easy on the Eyes\n",
      "1684\n",
      "A Gorgeous Girl Like Me\n",
      "1689\n",
      "MouseHunt\n",
      "1739\n",
      "Across to Singapore\n",
      "1760\n",
      "The Cherry Orchard\n",
      "1800\n",
      "Ryaba, My Chicken\n",
      "1814\n",
      "Cuban Rafters\n",
      "1821\n",
      "The Best Movie 3-DE\n",
      "1831\n",
      "Apartment 18\n",
      "1890\n",
      "Circo\n",
      "1911\n",
      "Mugabe and the White African\n",
      "2076\n",
      "A Man Who Was Superman\n",
      "2083\n",
      "Mechenosets\n",
      "2150\n",
      "Welcome Mr. President!\n",
      "2173\n",
      "The Spanish Apartment\n",
      "2176\n",
      "Kidnapping, Caucasian Style\n",
      "2198\n",
      "A Troll in Central Park\n",
      "2289\n",
      "Happy Weekend\n",
      "2302\n",
      "Kaiji 2: The Ultimate Gambler\n",
      "2399\n",
      "Amarkalam\n",
      "2423\n",
      "Hooked on the Game 2. The Next Level\n",
      "2498\n",
      "We Are from the Future 2\n",
      "2505\n",
      "The Rocket: The Legend of Rocket Richard\n",
      "2533\n",
      "Gabbar Is Back\n",
      "2567\n",
      "Louis C.K.: Live at the Beacon Theater\n",
      "2575\n",
      "My Old Classmate\n",
      "2645\n",
      "Kadhal Kottai\n",
      "2662\n",
      "Lift\n",
      "2686\n",
      "The Saint of Gamblers\n",
      "2759\n",
      "Saw: The Final Chapter\n",
      "2767\n",
      "The Visual Bible: The Gospel of John\n",
      "2795\n",
      "Pro Lyuboff\n",
      "2862\n",
      "The X Files: I Want to Believe\n",
      "2883\n",
      "Les Misérables\n",
      "2965\n"
     ]
    }
   ],
   "source": [
    "train_aud_rate =  aud_rate(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.at[2398, 'title'] = 'GUSKÔ BUDORI NO DENKI'\n",
    "test_set.at[3628, 'title'] = 'Barefoot'\n",
    "test_set.at[2425, 'title'] = 'La vérité si je mens! 3'\n",
    "test_set.at[33, 'title'] = 'Ayurveda: The Art of Being'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycling with Molière\n",
      "116\n",
      "House III: The Horror Show\n",
      "134\n",
      "Chelovek, kotoryy znal vsyo\n",
      "165\n",
      "Puella Magi Madoka Magica the Movie Part III: Rebellion\n",
      "217\n",
      "La caliente niña Julietta\n",
      "243\n",
      "Roadgames\n",
      "298\n",
      "Pokémon Heroes: Latios and Latias\n",
      "310\n",
      "French Women\n",
      "362\n",
      "Æon Flux\n",
      "366\n",
      "Hooked on the Game\n",
      "400\n",
      "Cats & Dogs 2 : The Revenge of Kitty Galore\n",
      "405\n",
      "Zombie Fever\n",
      "445\n",
      "Iron & Blood: The Legend of Taras Bulba\n",
      "492\n",
      "Yu-Gi-Oh! The Movie\n",
      "539\n",
      "Ticket to Vegas\n",
      "542\n",
      "A Nightmare on Elm Street Part 2: Freddy's Revenge\n",
      "588\n",
      "Italiano medio\n",
      "601\n",
      "The Second Jungle Book: Mowgli & Baloo\n",
      "610\n",
      "Ballplayer\n",
      "629\n",
      "The Tobacconist of Vallecas\n",
      "633\n",
      "Garv: Pride and Honour\n",
      "636\n",
      "Eternal Summer\n",
      "691\n",
      "Pokémon the Movie: Genesect and the Legend Awakened\n",
      "699\n",
      "Irudhi Suttru\n",
      "713\n",
      "Jails, Hospitals & Hip-Hop\n",
      "828\n",
      "Nasty Old People\n",
      "844\n",
      "Lights of New York\n",
      "875\n",
      "For Greater Glory - The True Story of Cristiada\n",
      "884\n",
      "Winnebago Man\n",
      "915\n",
      "F.C. De Kampioenen: Kampioen zijn blijft plezant\n",
      "939\n",
      "Erkan & Stefan 2\n",
      "973\n",
      "Escape to Victory\n",
      "988\n",
      "Shikshanachya Aaicha Gho\n",
      "1073\n",
      "Willy/Milly\n",
      "1104\n",
      "Joysticks\n",
      "1143\n",
      "Fist of the North Star: Legend of Raoh - Chapter of Death in Love\n",
      "1168\n",
      "Bipedalism\n",
      "1176\n",
      "Porky's 3: Revenge\n",
      "1230\n",
      "Aquí llega Condemor, el pecador de la pradera\n",
      "1271\n",
      "Torrente 5: Operación Eurovegas\n",
      "1341\n",
      "Horoscope for Good Luck\n",
      "1343\n",
      "Gladiator 1992\n",
      "1374\n",
      "95 Miles to Go\n",
      "1375\n",
      "The Texas Chain Saw Massacre\n",
      "1377\n",
      "Romanzo criminale\n",
      "1389\n",
      "Have Fun, Vasya!\n",
      "1390\n",
      "Long Live Freedom\n",
      "1406\n",
      "The Democratic Terrorist\n",
      "1408\n",
      "That still Karloson!\n",
      "1420\n",
      "Faster than Rabbits\n",
      "1456\n",
      "Ivan Tsarevich & the Grey Wolf 3\n",
      "1457\n",
      "Lovey-Dovey 2\n",
      "1477\n",
      "Agnivarsha: The Fire and the Rain\n",
      "1511\n",
      "Naturally Native\n",
      "1592\n",
      "Nunca en horas de clase\n",
      "1632\n",
      "Mongol: The Rise of Genghis Khan\n",
      "1951\n",
      "A Whole Life Ahead\n",
      "1959\n",
      "Antikiller 2: Antiterror\n",
      "1964\n",
      "The Friends at the Margherita Cafe\n",
      "1994\n",
      "Naruto Shippuden the Movie: Road to Ninja\n",
      "2055\n",
      "Lucky Island\n",
      "2084\n",
      "The 51st State\n",
      "2101\n",
      "Duniyadari\n",
      "2118\n",
      "SuperManager, or Hack of the Fate\n",
      "2119\n",
      "Vipers Nest\n",
      "2186\n",
      "Niko 2 - Little Brother, Big Trouble\n",
      "2330\n",
      "20th Century Boys - Chapter 1: Beginning of the End\n",
      "2376\n",
      "Maheshinte Prathikaaram\n",
      "2515\n",
      "Glukhar v kino\n",
      "2519\n",
      "This Is Not an Exit: The Fictional World of Bret Easton Ellis\n",
      "2544\n",
      "Jane Austen's Mafia!\n",
      "2613\n",
      "The Core\n",
      "2649\n",
      "The New Year's Rate Plan\n",
      "2685\n",
      "Pelli Choopulu\n",
      "2698\n",
      "Dr. Horrible's Sing-Along Blog\n",
      "2707\n",
      "Boj S Tenyu 2: Revansh\n",
      "2714\n",
      "Bablo\n",
      "2730\n",
      "The Matriarch\n",
      "2757\n",
      "The Chronicles of Riddick\n",
      "2761\n",
      "Fright Night Part 2\n",
      "2792\n",
      "Don't Even Think\n",
      "2946\n",
      "Dark World: Equilibrium\n",
      "3016\n",
      "Bad Guys Always Die\n",
      "3049\n",
      "Goliyon Ki Raasleela Ram-Leela\n",
      "3076\n",
      "Police Story 3: Supercop\n",
      "3088\n",
      "Vaaranam Aayiram\n",
      "3091\n",
      "Pokémon: The First Movie: Mewtwo Strikes Back\n",
      "3117\n",
      "Mugavari\n",
      "3122\n",
      "Twilight Online\n",
      "3310\n",
      "Five Senses of Eros\n",
      "3333\n",
      "The Bread, My Sweet\n",
      "3337\n",
      "Mommies, Happy New Year!\n",
      "3339\n",
      "Porn in the Hood\n",
      "3363\n",
      "The Nutcracker: The Untold Story\n",
      "3374\n",
      "Tosun Pasha\n",
      "3395\n",
      "Antikiller D.K\n",
      "3410\n",
      "Wabash Avenue\n",
      "3427\n",
      "Lucky Lady\n",
      "3449\n",
      "The Grace Lee Project\n",
      "3571\n",
      "Bizim Aile\n",
      "3579\n",
      "The Man Who Was Too Free\n",
      "3582\n",
      "Beyond the Mask\n",
      "3596\n",
      "Zathura: A Space Adventure\n",
      "3604\n",
      "Delhi-6\n",
      "3690\n",
      "Def Jam's How to Be a Player\n",
      "3713\n",
      "The Umbrella Woman\n",
      "3739\n",
      "Naan Kadavul\n",
      "3783\n",
      "Chaahat Ek Nasha...\n",
      "3803\n",
      "Around the World in Eighty Days\n",
      "3872\n",
      "No Retreat, No Surrender 2: Raging Thunder\n",
      "3877\n",
      "Harry, He's Here To Help\n",
      "3894\n",
      "What Men Still Talk About\n",
      "3895\n",
      "L'allenatore nel pallone 2\n",
      "3991\n",
      "Windstruck\n",
      "4081\n",
      "Battletruck\n",
      "4121\n",
      "The Precocious and Brief Life of Sabina Rivas\n",
      "4145\n",
      "Experience Preferred...But Not Essential\n",
      "4182\n",
      "Snowriders\n",
      "4340\n",
      "Those Magnificent Men in Their Flying Machines or How I Flew from London to Paris in 25 hours 11 minutes\n",
      "4346\n"
     ]
    }
   ],
   "source": [
    "test_aud_rate = aud_rate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "4398\n",
      "4398\n"
     ]
    }
   ],
   "source": [
    "print(len(train_aud_rate))\n",
    "print(len(train_set))\n",
    "print(len(test_aud_rate))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/Users/m-sim/Desktop/variables/train_aud_rate.pkl', 'wb')\n",
    "pickle.dump(train_aud_rate, f)\n",
    "f.close()\n",
    "f = open('C:/Users/m-sim/Desktop/variables/test_aud_rate.pkl', 'wb')\n",
    "pickle.dump(test_aud_rate, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['aud_rating'] = train_aud_rate\n",
    "test_set['aud_rating']  = test_aud_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtimefix(setname, colname='title'):\n",
    "    meantime = float(format(np.mean(setname.runtime),'.3f'))\n",
    "    for i in range(len(setname[colname])):\n",
    "        if pd.isnull(setname.runtime[i]):\n",
    "            try:\n",
    "                s_result = ia.get_movie(setname.at[i,'imdb_id'][2:])\n",
    "                the_unt = s_result[0]\n",
    "                ia.update(the_unt)\n",
    "                setname.at[i,'runtime'] = the_unt['runtime']\n",
    "            except:\n",
    "                setname.at[i,'runtime'] =  meantime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimefix(train_set)\n",
    "runtimefix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def budgetfix(setname, colname='imdb_id'):\n",
    "    mean = np.mean(setname.budget)\n",
    "    for imdbid in setname[colname]:\n",
    "        if setname.loc[setname[colname]==imdbid].budget.tolist()[0]==0:\n",
    "            print(imdbid)\n",
    "            row_index = setname.loc[setname[colname]==imdbid].index.tolist()[0]\n",
    "            try:\n",
    "                response = urlopen('https://www.imdb.com/title/'+imdbid)\n",
    "                soup = bs(response, 'xml')\n",
    "                budget = soup.find(\"span\", \"attribute\")\n",
    "                budget = budget.parent.get_text(strip=True)\n",
    "                budget = int(''.join([i for i in budget if i.isdigit()]))\n",
    "                setname.at[row_index,'budget'] = budget\n",
    "            except Exception as e:\n",
    "                setname.at[row_index,'budget'] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('D:/Study/Kaggle/tmdb-box-office-prediction/train_set.pkl', 'wb')\n",
    "pickle.dump(train_set, f)\n",
    "f.close()\n",
    "f = open('D:/Study/Kaggle/tmdb-box-office-prediction/test_set.pkl', 'wb')\n",
    "pickle.dump(test_set, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def belongs_to_col(setname,colname):\n",
    "#     collectionlist = setname[colname].tolist()\n",
    "\n",
    "#     newcollection = []\n",
    "#     for collection in collectionlist:\n",
    "#         if pd.isnull(collection):\n",
    "#             newcollection.append('nocollection')\n",
    "#         else:\n",
    "#             newcollection.append(collection.split(',')[1].split(':')[1].lstrip().replace(\"'\",''))\n",
    "\n",
    "\n",
    "#     setname[colname]=newcollection\n",
    "\n",
    "# belongs_to_col(train_set,'belongs_to_collection')\n",
    "# belongs_to_col(test_set,'belongs_to_collection')\n",
    "\n",
    "# train_set = pd.concat([train_set,pd.get_dummies(train_set['belongs_to_collection'])],axis=1)\n",
    "# train_set = train_set.drop(['belongs_to_collection','nocollection'],axis=1)\n",
    "\n",
    "# test_set = pd.concat([test_set,pd.get_dummies(test_set['belongs_to_collection'])],axis=1)\n",
    "# test_set = test_set.drop(['belongs_to_collection','nocollection'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belongscol(setname,colname='belongs_to_collection'):\n",
    "    collist=[]\n",
    "    for col in setname[colname]:\n",
    "        if pd.isnull(col):\n",
    "            collist.append(0)\n",
    "        else:\n",
    "            collist.append(1)\n",
    "    return collist\n",
    "\n",
    "train_set['belongs_to_collection'] = belongscol(train_set)\n",
    "test_set['belongs_to_collection'] = belongscol(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genrelist = train_set['genres'].tolist()\n",
    "\n",
    "newgenre = []\n",
    "for genre in genrelist:\n",
    "    if pd.isnull(genre):\n",
    "        newgenre.append(['nogenre'])\n",
    "    else:\n",
    "        genres=[]\n",
    "        for diffgenres in eval(genre):\n",
    "            genres.append(diffgenres['name'])\n",
    "        newgenre.append(genres)\n",
    "train_set['genres'] = newgenre\n",
    "genrelist = train_set['genres'].tolist()\n",
    "\n",
    "genrename = []\n",
    "for genres in genrelist:\n",
    "    if genres=='nogenre':\n",
    "        if genres not in genrename:\n",
    "            genrename.append(genres)\n",
    "    else:\n",
    "        for genre in genres:\n",
    "            if genre not in genrename:\n",
    "                genrename.append(genre)\n",
    "\n",
    "fullonehot=[]\n",
    "for genres in genrelist:\n",
    "    onehotlists=[]\n",
    "    for genre in genres:\n",
    "        onehot=[]\n",
    "        for genre_name in genrename:\n",
    "            if genre==genre_name:\n",
    "                onehot.append(1)\n",
    "            else:\n",
    "                onehot.append(0)\n",
    "        onehotlists.append(onehot)\n",
    "    fullonehot.append(np.bitwise_or.reduce(onehotlists))\n",
    "\n",
    "onehotdf = pd.DataFrame(fullonehot)\n",
    "onehotdf.columns=genrename\n",
    "train_set = pd.concat([train_set,onehotdf],axis=1)\n",
    "train_set = train_set.drop(['genres','nogenre'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionarycol(setname,colname):\n",
    "    collist = setname[colname].tolist()\n",
    "\n",
    "    newlist = []\n",
    "    for col in collist:\n",
    "        if pd.isnull(col):\n",
    "            newlist.append(['none'])\n",
    "        else:\n",
    "            names=[]\n",
    "            for diffcol in eval(col):\n",
    "                names.append(diffcol['name'])\n",
    "            newlist.append(names)\n",
    "    setname[colname] = newlist\n",
    "    newlist = setname[colname].tolist()\n",
    "    \n",
    "    savedname = []\n",
    "    for name in newlist:\n",
    "        if name=='none':\n",
    "            if name not in savedname:\n",
    "                savedname.append(name)\n",
    "        else:\n",
    "            for names in name:\n",
    "                if names not in savedname:\n",
    "                    savedname.append(names)\n",
    "    fullonehot=[]\n",
    "    for name in newlist:\n",
    "        onehotlists=[]\n",
    "        for names in name:\n",
    "            onehot=[]\n",
    "            for sname in savedname:\n",
    "                if names==sname:\n",
    "                    onehot.append(1)\n",
    "                else:\n",
    "                    onehot.append(0)\n",
    "            onehotlists.append(onehot)\n",
    "        fullonehot.append(np.bitwise_or.reduce(onehotlists))\n",
    "    onehotdf = pd.DataFrame(fullonehot)\n",
    "    onehotdf.columns=savedname\n",
    "    return onehotdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testonehotdf = dictionarycol(test_set,'genres')\n",
    "test_set = pd.concat([test_set,onehotdf],axis=1)\n",
    "test_set = test_set.drop(['genres'],axis=1)\n",
    "\n",
    "\n",
    "# onehotdf = dictionarycol(train_set,'production_companies')\n",
    "# testonehotdf = dictionarycol(test_set,'production_companies')\n",
    "\n",
    "# train_set = pd.concat([train_set,onehotdf],axis=1)\n",
    "train_set = train_set.drop(['production_companies'],axis=1)\n",
    "# test_set = pd.concat([test_set,onehotdf],axis=1)\n",
    "test_set = test_set.drop(['production_companies'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_country(setname,colname):\n",
    "    collist = setname[colname].tolist()\n",
    "\n",
    "    newlist = []\n",
    "    for col in collist:\n",
    "        if pd.isnull(col):\n",
    "            newlist.append('none')\n",
    "        else:\n",
    "            names=[]\n",
    "            for diffcol in eval(col):\n",
    "                names.append(diffcol['name'])\n",
    "            newlist.append(names[0])\n",
    "    setname[colname] = newlist\n",
    "    newlist = setname[colname].tolist()\n",
    "\n",
    "production_country(train_set,'production_countries')\n",
    "production_country(test_set,'production_countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countrytonum (setname,colname='production_countries'):\n",
    "    vallist=[]\n",
    "    for val in setname[colname]:\n",
    "        if val!='United States of America':\n",
    "            vallist.append(1)\n",
    "        else:\n",
    "            vallist.append(2)\n",
    "    return vallist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['production_countries'] = countrytonum(train_set)\n",
    "test_set['production_countries'] = countrytonum(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def director(setname,colname):\n",
    "\n",
    "    collist = setname[colname].tolist()\n",
    "\n",
    "    newlist = []\n",
    "    \n",
    "    for col in collist:\n",
    "        if pd.isnull(col):\n",
    "            newlist.append('none')\n",
    "        else:\n",
    "            names=[]\n",
    "            for diffcol in eval(col):\n",
    "                if diffcol['job']=='Director':\n",
    "                    names.append(diffcol['name'])\n",
    "            if names==[]:\n",
    "                newlist.append('none')\n",
    "            else:\n",
    "                newlist.append(names[0])\n",
    "    setname['director'] = newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director(train_set, 'crew')\n",
    "train_set = train_set.drop(['crew'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director(test_set,'crew')\n",
    "test_set = test_set.drop(['crew'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = pd.concat([train_set,pd.get_dummies(train_set['director'])],axis=1)\n",
    "# test_set = pd.concat([test_set,pd.get_dummies(test_set['director'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast(setname, colname):\n",
    "    collist = setname[colname].tolist()\n",
    "\n",
    "    newlist = []\n",
    "    for col in collist:\n",
    "        if col=='[]' or pd.isnull(col):\n",
    "            newlist.append('none')\n",
    "        else:\n",
    "            names=[]\n",
    "            for diffcol in eval(col):\n",
    "                names.append(diffcol['name'])\n",
    "            newlist.append(names[0])\n",
    "    setname[colname]=newlist\n",
    "\n",
    "cast(train_set,'cast')\n",
    "cast(test_set,'cast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = pd.concat([train_set,pd.get_dummies(train_set['cast'])],axis=1)\n",
    "# test_set = pd.concat([test_set,pd.get_dummies(test_set['cast'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set = pd.concat([train_set,pd.get_dummies(train_set['original_language'])],axis=1)\n",
    "#test_set = pd.concat([test_set,pd.get_dummies(test_set['original_language'])],axis=1)\n",
    "#train_set = pd.concat([train_set,pd.get_dummies(train_set['production_countries'])],axis=1)\n",
    "#test_set = pd.concat([test_set,pd.get_dummies(test_set['production_countries'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang(setname,colname):\n",
    "    langlist=[]\n",
    "    for lang in setname[colname]:\n",
    "        if lang!='en':\n",
    "            langlist.append(0)\n",
    "        else:\n",
    "            langlist.append(1)\n",
    "    setname[colname]=langlist\n",
    "\n",
    "lang(train_set,'original_language')\n",
    "lang(test_set,'original_language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def country(setname,colname):\n",
    "#    countrylist=[]\n",
    "#    for count in train_set['production_countries']:\n",
    "#        if count!=['United States of America']:\n",
    "#            countrylist.append(0)\n",
    "#        else:\n",
    "#            countrylist.append(1)\n",
    "#    train_set['production_countries']=countrylist\n",
    "        \n",
    "#country(train_set,'production_countries')\n",
    "#country(test_set,'production_countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.drop(['homepage','overview','spoken_languages','tagline','poster_path','cast','director','original_title','Keywords','status'], axis=1)\n",
    "test_set = test_set.drop(['nogenre','homepage','overview','spoken_languages','tagline','poster_path','cast','director','original_title','Keywords','status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_col = train_set.columns.tolist()\n",
    "# test_set_col = test_set.columns.tolist()\n",
    "\n",
    "# for i in train_set_col:\n",
    "#     for l in test_set_col:\n",
    "#         if i==l:\n",
    "#             test_set_col.remove(l)\n",
    "\n",
    "# len(pd.concat([train_set,test_set[test_set_col]],axis=1).columns)\n",
    "\n",
    "# train_set_col = train_set.columns.tolist()\n",
    "# test_set_col = test_set.columns.tolist()\n",
    "\n",
    "# for i in test_set_col:\n",
    "#     for l in train_set_col:\n",
    "#         if i==l:\n",
    "#             train_set_col.remove(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.drop(['id','revenue','imdb_id','release_date','title'], axis=1).copy()\n",
    "y = train_set['revenue'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X.fillna(0, inplace=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=500, min_samples_leaf=5, random_state=42)\n",
    "forest_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "np.sqrt(mean_squared_log_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "msle_scorer = make_scorer(mean_squared_log_error)\n",
    "\n",
    "scores = cross_val_score(forest_reg, X, y,\n",
    "                         scoring=msle_scorer, cv=10)\n",
    "forrest_rmsle = np.sqrt(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(forrest_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [800, 850], 'max_features': [1], 'min_samples_leaf':[1,5], 'min_samples_split':[2,5]}\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring=msle_scorer, return_train_score=True)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(grid_search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_set.drop(['id','imdb_id','release_date','title'], axis=1).copy()\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "test_y_pred = forest_reg.predict(X)\n",
    "\n",
    "submission_df = test_set['id'].copy()\n",
    "submission_df = pd.DataFrame(submission_df)\n",
    "submit_df = submission_df[:4398]\n",
    "submit_df['revenue'] = test_y_pred\n",
    "submit_df.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
